{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum Machine Learning Model @ TSpark 2022 Quantum+ Camp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import deepchem as dc\n",
    "import tensorcircuit as tc\n",
    "import time\n",
    "\n",
    "tc.set_dtype(\"complex128\")\n",
    "K = tc.set_backend(\"tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Gobal Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10 #number of qbits\n",
    "nlayers = 5 #layers of operation\n",
    "thetas = K.randn([n,nlayers]) #rotation angle of each rx gates\n",
    "epochs = 50 #number of time of training\n",
    "lr = 0.01 #learning rate, changed later\n",
    "batch_size = 64 #number of data calculated in each batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks, datasets, transformers = dc.molnet.load_tox21(featurizer='ECFP')\n",
    "train_dataset, valid_dataset, test_dataset = datasets\n",
    "\n",
    "w_col = train_dataset.w[:, 1]\n",
    "train_num = np.nonzero(w_col)\n",
    "x_train=(train_dataset.X[train_num]*2-1)/32\n",
    "y_train=train_dataset.y[train_num][:, 1]\n",
    "w_train=train_dataset.w[train_num][:, 1]\n",
    "\n",
    "w_col = valid_dataset.w[:,1]\n",
    "val_num = np.nonzero(w_col)\n",
    "x_val=(valid_dataset.X[val_num]*2-1)/32\n",
    "y_val=valid_dataset.y[val_num][:, 1]\n",
    "w_val=valid_dataset.w[val_num][:, 1]\n",
    "\n",
    "w_col = test_dataset.w[:,1]\n",
    "test_num = np.nonzero(w_col)\n",
    "x_test=(test_dataset.X[test_num]*2-1)/32\n",
    "y_test=test_dataset.y[test_num][:, 1]\n",
    "w_test=test_dataset.w[test_num][:, 1]\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train, w_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val, w_val))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test, w_test))\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_sumz(param, x):#covert qbits with x via gates with param to a value\n",
    "    c = tc.Circuit(n, inputs=x)\n",
    "    for j in range(nlayers):\n",
    "        for i in range(n):\n",
    "            c.rx(i, theta=param[i, j])\n",
    "        if(j%2==1):\n",
    "            for i in range(n - 1):\n",
    "                c.cnot(i, i + 1)\n",
    "        else:\n",
    "            for i in range(n - 1):\n",
    "                c.cnot(n-1-i, n-2-i)\n",
    "    \n",
    "    return K.real(K.sum([c.expectation_ps(z=[i]) for i in range(n)]))\n",
    "\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits = False) #defining loss function, using Binary Cross Entropy\n",
    "\n",
    "def loss(param, X, y, w): #predict using x and return the weighted loss value\n",
    "    logits = exp_sumz(param, X)\n",
    "    y = K.reshape(y, [1])\n",
    "    logits = K.reshape(logits, [1])\n",
    "    logits = K.sigmoid(logits)\n",
    "    loss = loss_fn(y, logits, sample_weight=w)\n",
    "    return loss, logits, y, w\n",
    "\n",
    "exp_sumz_batch_grad = K.vectorized_value_and_grad(loss, vectorized_argnums=(1,2,3), argnums=(0), has_aux=True) #vectorize and audo gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "  print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "  result = []\n",
    "  loss_d = []\n",
    "  data_should = []\n",
    "  data_w = []\n",
    "  if(epoch>=1):\n",
    "      lr = 1e-4\n",
    "  ttt = time.time()\n",
    "  for id, (x_batch_train, y_batch_train, w_batch_train) in enumerate(train_dataset):\n",
    "    #result = np.array()\n",
    "    if(x_batch_train.shape[0]!=batch_size):\n",
    "        break\n",
    "    loss, slope = exp_sumz_batch_grad(thetas, x_batch_train, y_batch_train, w_batch_train)\n",
    "    result.append(loss[1])\n",
    "    data_should.append(loss[2])\n",
    "    thetas -= slope*lr\n",
    "    loss_d.append(loss[0])\n",
    "    data_w.append(loss[3])\n",
    "  ttt = time.time()-ttt\n",
    "  print(\"Time spent %f\" % (float(ttt,)))\n",
    "  data_w = K.stack(data_w)\n",
    "  data_w = K.reshape(data_w, [data_w.shape[0]*data_w.shape[1]])\n",
    "  result_loss = K.mean(loss_d)\n",
    "  print(\"Loss %f\" % (float(result_loss),))\n",
    "  result = K.stack(result)\n",
    "  result = K.reshape(result, [result.shape[0]*result.shape[1]])\n",
    "  data_should = K.stack(data_should)\n",
    "  data_should = K.reshape(data_should, [data_should.shape[0]*data_should.shape[1]])\n",
    "  auc_m = tf.keras.metrics.AUC(from_logits = False)\n",
    "  auc_m.update_state(data_should,result,sample_weight=data_w)\n",
    "  print(\"AUC %f\" % (auc_m.result().numpy()),)\n",
    "  print(\"\\nStart val of epoch %d\" % (epoch,))\n",
    "\n",
    "\n",
    "  result = []\n",
    "  loss_d = []\n",
    "  data_should = []\n",
    "  data_w = []\n",
    "  for id, (x_batch_train, y_batch_train, w_batch_train) in enumerate(val_dataset):\n",
    "    if(x_batch_train.shape[0]!=batch_size):\n",
    "      break\n",
    "    loss, slope = exp_sumz_batch_grad(thetas, x_batch_train, y_batch_train, w_batch_train)\n",
    "    result.append(loss[1])\n",
    "    data_should.append(loss[2])\n",
    "    loss_d.append(loss[0])\n",
    "    data_w.append(loss[3])\n",
    "  data_w = K.stack(data_w)\n",
    "  data_w = K.reshape(data_w, [data_w.shape[0]*data_w.shape[1]])\n",
    "  result_loss = K.mean(loss_d)\n",
    "  print(\"Loss %f\" % (float(result_loss),))\n",
    "  result = K.stack(result)\n",
    "  result = K.reshape(result, [result.shape[0]*result.shape[1]])\n",
    "  data_should = K.stack(data_should)\n",
    "  data_should = K.reshape(data_should, [data_should.shape[0]*data_should.shape[1]])\n",
    "  auc_m = tf.keras.metrics.AUC(from_logits = False)\n",
    "  auc_m.update_state(data_should,result,sample_weight=data_w)\n",
    "  print(\"AUC %f\" % (auc_m.result().numpy()),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nbegin testing\")\n",
    "result = []\n",
    "loss_d = []\n",
    "data_should = []\n",
    "data_w = []\n",
    "for id, (x_batch_train, y_batch_train, w_batch_train) in enumerate(test_dataset):\n",
    "    if(x_batch_train.shape[0]!=batch_size):\n",
    "        break\n",
    "    loss, slope = exp_sumz_batch_grad(thetas, x_batch_train, y_batch_train, w_batch_train)\n",
    "    result.append(loss[1])\n",
    "    data_should.append(loss[2])\n",
    "    loss_d.append(loss[0])\n",
    "    data_w.append(loss[3])\n",
    "data_w = K.stack(data_w)\n",
    "data_w = K.reshape(data_w, [data_w.shape[0]*data_w.shape[1]])\n",
    "result_loss = K.mean(loss_d)\n",
    "print(\"Loss %f\" % (float(result_loss),))\n",
    "result = K.stack(result)\n",
    "result = K.reshape(result, [result.shape[0]*result.shape[1]])\n",
    "data_should = K.stack(data_should)\n",
    "data_should = K.reshape(data_should, [data_should.shape[0]*data_should.shape[1]])\n",
    "auc_m = tf.keras.metrics.AUC(from_logits = False)\n",
    "auc_m.update_state(data_should,result,sample_weight=data_w)\n",
    "print(\"AUC %f\" % (auc_m.result().numpy()),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the given parameters, the testing result is with AUC below 0.7.\n",
    "The time spent is more than 90 second for the first batch in the first epoch."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5e8f1d8c076c9bb3313eac266411f5493046fe213e43b04a760cec028c9f2f2b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tencent_tensorcircuit')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
